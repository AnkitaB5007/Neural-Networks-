{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu8JxbTAhNNU"
      },
      "source": [
        "Microsoft Teams Name - Tanvi Ajay Gunjal, Ankita Behura Name - Tanvi Ajay Gunjal Matriculation Number - 7002984 Name - Ankita Behura Matriculation Number - 7002639 Tutor - Redion Xhepa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPPEnChkhNNn"
      },
      "outputs": [],
      "source": [
        "\"\"\"multilayer_perceptron.py\n",
        "\n",
        "A Multilayer Perceptron implementation example using Pytorch.\n",
        "This example does handwritten digit recognition using the MNIST database.\n",
        "(http://yann.lecun.com/exdb/mnist/)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from scipy.ndimage.interpolation import shift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cW8x1M8hNNx"
      },
      "outputs": [],
      "source": [
        "# Network hyperparameters\n",
        "input_size = 28*28\n",
        "hidden_layer_size= 512\n",
        "output_size = 10\n",
        "num_epochs = 2\n",
        "batch_size = 100\n",
        "learning_rate = 1e-3\n",
        "learning_rate_decay = 0.8\n",
        "num_workers = 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I63Pyj3YhNN4"
      },
      "source": [
        "Add random noise (or any distribution that you think might be useful). Comment\n",
        "on the amount of noise and how it affects the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8zVW51ihNN_"
      },
      "outputs": [],
      "source": [
        "# Add random Noise to data\n",
        "def add_noise(img):\n",
        "    noise = torch.randn(img.size()) * 0.2\n",
        "    noisy_img = img + noise\n",
        "    return noisy_img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mc8lC-fbhNOD"
      },
      "source": [
        "Shift the images in all four directions by the given."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rfHtPn0hNOF"
      },
      "outputs": [],
      "source": [
        "# Method to shift the image by given dimension\n",
        "train_transform=transforms.Compose([\n",
        "    transforms.RandomApply([transforms.RandomAffine(degrees=(0, 0), \n",
        "                                  translate=(0.10, 0.10))], p=0.20),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkUYMb5xhNOI"
      },
      "outputs": [],
      "source": [
        "# Load MNIST train and test datasets \n",
        "def load_dataset():\n",
        "    train_data = datasets.MNIST(root='data', train=True,\n",
        "                                   download=True, transform=train_transform) # Added Data augmentation transform defined above\n",
        "    test_data = datasets.MNIST(root='data', train=False,\n",
        "                                  download=True, transform=transforms.ToTensor())\n",
        "    \n",
        "    test_size = len(test_data)\n",
        "    \n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "    num_workers=num_workers)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
        "    num_workers=num_workers)\n",
        "    return train_loader,test_loader,test_size\n",
        "\n",
        "train_loader,test_loader,test_size = load_dataset()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cuVfKXwhNOQ"
      },
      "outputs": [],
      "source": [
        "# function to update learning rate\n",
        "def update_lr(optimizer, lr):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgVVU6TDhNOT"
      },
      "outputs": [],
      "source": [
        "# Create model \n",
        "class MLP(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        \n",
        "        # Input layer\n",
        "        self.fc1 = nn.Linear(input_size, hidden_layer_size)\n",
        "        \n",
        "        # Hidden layer\n",
        "        self.fc2 = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
        "        \n",
        "        # Output layer\n",
        "        self.fc3 = nn.Linear(hidden_layer_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(-1, input_size)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71ipqEzBhNOd",
        "outputId": "91703f05-a39a-4acc-8d93-29cdc1287a84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP(\n",
            "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
            "  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "Epoch [1/2], Step [50/600], Loss: 2.3148\n",
            "Epoch [1/2], Step [100/600], Loss: 2.2659\n",
            "Epoch [1/2], Step [150/600], Loss: 2.2591\n",
            "Epoch [1/2], Step [200/600], Loss: 2.2625\n",
            "Epoch [1/2], Step [250/600], Loss: 2.2309\n",
            "Epoch [1/2], Step [300/600], Loss: 2.2141\n",
            "Epoch [1/2], Step [350/600], Loss: 2.1962\n",
            "Epoch [1/2], Step [400/600], Loss: 2.1836\n",
            "Epoch [1/2], Step [450/600], Loss: 2.1538\n",
            "Epoch [1/2], Step [500/600], Loss: 2.1365\n",
            "Epoch [1/2], Step [550/600], Loss: 2.1080\n",
            "Epoch [1/2], Step [600/600], Loss: 2.1021\n",
            "Epoch [2/2], Step [50/600], Loss: 2.0675\n",
            "Epoch [2/2], Step [100/600], Loss: 2.0079\n",
            "Epoch [2/2], Step [150/600], Loss: 2.0303\n",
            "Epoch [2/2], Step [200/600], Loss: 2.0352\n",
            "Epoch [2/2], Step [250/600], Loss: 1.9938\n",
            "Epoch [2/2], Step [300/600], Loss: 1.9790\n",
            "Epoch [2/2], Step [350/600], Loss: 1.9301\n",
            "Epoch [2/2], Step [400/600], Loss: 1.9106\n",
            "Epoch [2/2], Step [450/600], Loss: 1.8759\n",
            "Epoch [2/2], Step [500/600], Loss: 1.8674\n",
            "Epoch [2/2], Step [550/600], Loss: 1.8700\n",
            "Epoch [2/2], Step [600/600], Loss: 1.8275\n"
          ]
        }
      ],
      "source": [
        "# Initialize model\n",
        "model = MLP()\n",
        "print(model)\n",
        "\n",
        "\n",
        "# Specify loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Specify optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train\n",
        "model.train()\n",
        "total_step = len(train_loader)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Loading each input batch\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "               \n",
        "        # Add some noise to the image\n",
        "        noisy_img = add_noise(images)\n",
        "\n",
        "        # Outputs after forward pass\n",
        "        outputs = model(noisy_img)       \n",
        "        \n",
        "         # Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backprop to update model parameters \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 50 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "    # Update learning rate for next epoch       \n",
        "    learning_rate *= learning_rate_decay\n",
        "    update_lr(optimizer, learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1MJCJjOhNOm",
        "outputId": "3a665bd6-d61d-472d-afda-7a3201c5cf1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the MLP on 10000 test images: 71.37 %\n"
          ]
        }
      ],
      "source": [
        "# Test\n",
        "model.eval()\n",
        "correct=0\n",
        "for images, labels in test_loader:\n",
        "\n",
        "    # Compute predicted outputs (forward pass)\n",
        "    output = model(images)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = criterion(output, labels)\n",
        "\n",
        "    # Convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)\n",
        "\n",
        "    # compare predictions to true labels\n",
        "    correct += (pred == labels).sum().item()\n",
        "    \n",
        "\n",
        "# Test accuracy\n",
        "print('Accuracy of the MLP on {} test images: {} %'.format(test_size, 100 * (correct / test_size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kAWDULOhNOp"
      },
      "source": [
        "Running the above model resulted in accuracy of 49% . Adding some random noise and Random Affine transform on our train data\n",
        "(data augmentation) gave us loss value as low as 1.8275, which shows some improvement."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}